{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53204125",
   "metadata": {},
   "source": [
    "# Collect and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625aef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 363 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Directory paths\n",
    "data_dir = \"C:\\\\Users\\\\dalkh\\\\OneDrive\\\\Desktop\\\\saif folder\\\\project\\\\Real time monitoring construction activites project\\construction image\"\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e0db5",
   "metadata": {},
   "source": [
    "# Build a Convolutional Neural Network (CNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f2c3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalkh\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(4, activation='softmax')  # Assuming 5 different stages of construction\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228765dd",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d660eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalkh\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 12/100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 1s/step - accuracy: 0.3081 - loss: 2.0535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalkh\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 399ms/step - accuracy: 0.3061 - loss: 1.7972 - val_accuracy: 0.4435 - val_loss: 1.2575\n",
      "Epoch 2/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 343ms/step - accuracy: 0.4827 - loss: 1.1460 - val_accuracy: 0.5923 - val_loss: 0.9455\n",
      "Epoch 3/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 353ms/step - accuracy: 0.5465 - loss: 1.0472 - val_accuracy: 0.6198 - val_loss: 0.9056\n",
      "Epoch 4/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 358ms/step - accuracy: 0.5499 - loss: 0.9911 - val_accuracy: 0.6529 - val_loss: 0.8946\n",
      "Epoch 5/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 341ms/step - accuracy: 0.6303 - loss: 0.8698 - val_accuracy: 0.6639 - val_loss: 0.7536\n",
      "Epoch 6/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 359ms/step - accuracy: 0.6539 - loss: 0.8272 - val_accuracy: 0.6584 - val_loss: 0.8020\n",
      "Epoch 7/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 356ms/step - accuracy: 0.6560 - loss: 0.8125 - val_accuracy: 0.6749 - val_loss: 0.8344\n",
      "Epoch 8/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 353ms/step - accuracy: 0.6692 - loss: 0.7912 - val_accuracy: 0.6915 - val_loss: 0.6934\n",
      "Epoch 9/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 352ms/step - accuracy: 0.7098 - loss: 0.7416 - val_accuracy: 0.6612 - val_loss: 0.8307\n",
      "Epoch 10/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 311ms/step - accuracy: 0.6910 - loss: 0.7259 - val_accuracy: 0.7300 - val_loss: 0.6338\n",
      "Epoch 11/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 343ms/step - accuracy: 0.7033 - loss: 0.7212 - val_accuracy: 0.6970 - val_loss: 0.6948\n",
      "Epoch 12/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 348ms/step - accuracy: 0.7106 - loss: 0.6803 - val_accuracy: 0.7686 - val_loss: 0.5942\n",
      "Epoch 13/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 347ms/step - accuracy: 0.7424 - loss: 0.6107 - val_accuracy: 0.7796 - val_loss: 0.5959\n",
      "Epoch 14/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 353ms/step - accuracy: 0.7287 - loss: 0.6726 - val_accuracy: 0.7713 - val_loss: 0.5911\n",
      "Epoch 15/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 337ms/step - accuracy: 0.7325 - loss: 0.5868 - val_accuracy: 0.7355 - val_loss: 0.5994\n",
      "Epoch 16/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 342ms/step - accuracy: 0.7409 - loss: 0.6115 - val_accuracy: 0.7355 - val_loss: 0.7270\n",
      "Epoch 17/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 355ms/step - accuracy: 0.7185 - loss: 0.6942 - val_accuracy: 0.7383 - val_loss: 0.6046\n",
      "Epoch 18/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 347ms/step - accuracy: 0.7006 - loss: 0.6249 - val_accuracy: 0.7824 - val_loss: 0.5370\n",
      "Epoch 19/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 340ms/step - accuracy: 0.8085 - loss: 0.5265 - val_accuracy: 0.8044 - val_loss: 0.4847\n",
      "Epoch 20/20\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 334ms/step - accuracy: 0.8014 - loss: 0.5389 - val_accuracy: 0.7961 - val_loss: 0.4643\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=20,\n",
    "                validation_data=train_generator,\n",
    "    validation_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd4bf0",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e204bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.7868 - loss: 0.5381\n",
      "Accuracy: 79.89%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(train_generator)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615ea73",
   "metadata": {},
   "source": [
    "# Implement the Image Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd279f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_construction_stage(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (150, 150))\n",
    "    img = img / 255.0\n",
    "    img = img.reshape(1, 150, 150, 3)\n",
    "    \n",
    "    prediction = model.predict(img)\n",
    "    stage = train_generator.class_indices\n",
    "    predicted_stage = max(stage, key=lambda key: prediction[0][stage[key]])\n",
    "    \n",
    "    return predicted_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ad64a",
   "metadata": {},
   "source": [
    "# Object Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1241c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    # Load YOLO model\n",
    "    net = cv2.dnn.readNet(\"C:\\\\Users\\\\dalkh\\\\OneDrive\\\\Desktop\\\\saif folder\\\\project\\\\Real time monitoring construction activites project\\\\yolo\\\\yolov3.weights\", \n",
    "                           \"C:\\\\Users\\\\dalkh\\\\OneDrive\\\\Desktop\\\\saif folder\\\\project\\\\Real time monitoring construction activites project\\\\yolo\\\\yolov3.cfg.cfg\")\n",
    "\n",
    "    # Load the COCO class labels\n",
    "    with open(\"C:\\\\Users\\\\dalkh\\\\OneDrive\\\\Desktop\\\\saif folder\\\\project\\\\Real time monitoring construction activites project\\\\yolo\\\\coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    detected_objects = []\n",
    "    \n",
    "    # Lists for detected bounding boxes, confidences, and class IDs\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    # Process each detection\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            # Only consider strong detections\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                # Append bounding box, confidence, and class ID\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # Apply Non-Maxima Suppression to remove duplicates\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "    # Draw bounding boxes and labels on the image\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            detected_objects.append(label)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Label the detected object\n",
    "            cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    # Display the image with detected objects\n",
    "    cv2.imshow(\"Object Detection\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return detected_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51932f",
   "metadata": {},
   "source": [
    "# Compare with Previous Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1c2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_previous(image_path, previous_stage):\n",
    "    current_stage = predict_construction_stage(image_path)\n",
    "    \n",
    "    if current_stage is None:\n",
    "        print(\"Error: Could not predict the current stage of construction.\")\n",
    "        return None\n",
    "    \n",
    "    if current_stage != previous_stage:\n",
    "        print(f\"Progress detected: Moved from {previous_stage} to {current_stage}.\")\n",
    "    else:\n",
    "        print(\"No progress detected.\")\n",
    "    \n",
    "    return current_stage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d412c3b",
   "metadata": {},
   "source": [
    "# Handle Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1c7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image(image_path, expected_stage):\n",
    "    predicted_stage = predict_construction_stage(image_path)\n",
    "    if predicted_stage != expected_stage:\n",
    "        raise ValueError(f\"Error: Uploaded image does not match the expected construction stage. Expected {expected_stage} but got {predicted_stage}.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b33f7",
   "metadata": {},
   "source": [
    "# Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "823d3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(image_path, expected_stage, current_stage, detected_objects, model_accuracy,image_description):\n",
    "    report = FPDF()\n",
    "    report.add_page()\n",
    "\n",
    "    report.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    # Title\n",
    "    report.set_font(\"Arial\", size=16)\n",
    "    report.cell(200, 10, txt=\"Construction Site Monitoring Report\", ln=True, align=\"C\")\n",
    "    \n",
    "    # Image Path\n",
    "    report.set_font(\"Arial\", size=12)\n",
    "    report.cell(200, 10, txt=f\"Image Path: {image_path}\", ln=True)\n",
    "    \n",
    "    # Image Description\n",
    "    report.cell(200, 10, txt=\"Image Description:\", ln=True)\n",
    "    report.multi_cell(0, 10, txt=image_description)\n",
    "    \n",
    "    # Expected vs Actual Stage\n",
    "    report.cell(200, 10, txt=f\"Expected Stage: {expected_stage}\", ln=True)\n",
    "    report.cell(200, 10, txt=f\"Detected Stage: {current_stage}\", ln=True)\n",
    "    \n",
    "    # Detected Objects\n",
    "    report.cell(200, 10, txt=\"Detected Objects:\", ln=True)\n",
    "    for obj in detected_objects:\n",
    "        report.cell(200, 10, txt=f\"- {obj}\", ln=True)\n",
    "        \n",
    "    # Model Accuracy\n",
    "    report.cell(200, 10, txt=f\"Model Accuracy: {model_accuracy*100:.2f}%\", ln=True)    \n",
    "           \n",
    "    # Save Report\n",
    "    report_output_path = \"construction_report.pdf\"\n",
    "    report.output(report_output_path)\n",
    "    print(f\"Report generated: {report_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462e8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_description = \"This image shows the facade stage of construction, where exterior finishing is taking place, and scaffolding is visible around the building.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430491b",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
      "Error: Uploaded image does not match the expected construction stage. Expected foundation but got facade.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Progress detected: Moved from interior to facade.\n",
      "Current stage of construction: facade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Simulate the user uploading an image\n",
    "    image_path = \"C:\\\\Users\\\\dalkh\\\\OneDrive\\\\Desktop\\\\test foundation image.jpeg\"\n",
    "    expected_stage = \"foundation\"  # Example of what the user believes the stage to be\n",
    "\n",
    "    # Validate the image against the expected stage\n",
    "    validate_image(image_path, expected_stage)\n",
    "    print(f\"Image validated successfully as {expected_stage}.\")\n",
    "    \n",
    "except ValueError as ve:\n",
    "    print(ve)\n",
    "    \n",
    "finally:    \n",
    "    # Compare with a previous image (for progress checking)\n",
    "    previous_image_path = \"C:\\\\Users\\\\dalkh\\\\OneDrive\\\\Desktop\\\\construction image\\\\foundation\\\\download (1).jpeg\"\n",
    "    previous_stage = predict_construction_stage(previous_image_path)\n",
    "    \n",
    "    # Compare the new image with the previous one\n",
    "    current_stage = compare_with_previous(image_path, previous_stage)\n",
    "    print(f\"Current stage of construction: {current_stage}\")\n",
    "    \n",
    "    # Object detection\n",
    "    detected_objects = detect_objects(image_path)\n",
    "\n",
    "  # Generate the report\n",
    "    generate_report(image_path, expected_stage, current_stage, detected_objects, model_accuracy, image_description) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5adb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
